{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5NDXrN2CtH7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSoDSZ9BUbtr",
    "outputId": "ebd315ca-a199-4c4a-d805-066eeaa6a2dd"
   },
   "outputs": [],
   "source": [
    "### load the meta data - USING DOWNLOADED DATASET\n",
    "\n",
    "data = []\n",
    "with gzip.open('../data/reviews_Video_Games_5.json.gz') as f:\n",
    "    for l in f:\n",
    "        data.append(json.loads(l.strip()))\n",
    "    \n",
    "# total length of list, this number equals total number of products\n",
    "print(len(data))\n",
    "\n",
    "# first row of the list\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "knpHP22w4scK",
    "outputId": "c019bc40-a7a6-47fc-d3ea-ee12ed0bfe63"
   },
   "outputs": [],
   "source": [
    "# convert list into pandas dataframe\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "print(len(df))\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.loc[:,'reviewText']\n",
    "y = df.loc[:,'overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING OUTPUTS\n",
    "y_new = []\n",
    "for i in range(y.size):\n",
    "    if y[i] < 4:\n",
    "        y_new.append(0) #bad\n",
    "    else:\n",
    "        y_new.append(1) #good\n",
    "\n",
    "print(y_new.count(1))\n",
    "print(y_new.count(0))\n",
    "print(y_new.count(1)/y_new.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING INPUTS\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "for i in range(x.size):\n",
    "    x[i] = x[i].lower()\n",
    "    x[i] = REPLACE_BY_SPACE_RE.sub(' ', x[i]) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    x[i] = BAD_SYMBOLS_RE.sub('', x[i]) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    x[i] = ' '.join(word for word in x[i].split() if word not in STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATAFRAME\n",
    "list_of_data = list(zip(x, y_new))\n",
    "df_processed = pd.DataFrame(list_of_data, columns=['input','output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_processed.input\n",
    "y = df_processed.output\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD2VEC\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "corpus = []\n",
    "for i in range(x.size):\n",
    "    corpus.append(x[i])\n",
    "\n",
    "all_words = [nltk.word_tokenize(sent) for sent in corpus]\n",
    "\n",
    "# model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model = Word2Vec(sentences=all_words, min_count=2, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING Store just the words + their trained embeddings.\n",
    "from gensim.models import KeyedVectors\n",
    "word_vectors = word2vec.wv\n",
    "word_vectors.save(\"word2vec.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not exactly sure how to model it... or if I modeled it correctly at the #WORD2VEC part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "parse_and_clean_meta_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
